{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Em-UWDycsmVB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-XItV_bOAho",
        "outputId": "f6caf03b-b3c5-4f34-8c91-63c1aeaea42e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "# directly do sentiment-analysis without choosing any transformer. It will choose some default.\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y3h8_EEn1i6",
        "outputId": "1e20667b-732e-424f-c3cd-0bd75ee27fad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chosing specific transformer for tokenizing the input words. This tokenized input words should only be fed into same transformer and not to the other transformer. "
      ],
      "metadata": {
        "id": "5h4eSTKioSBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# preparing the tokenizer by picking the model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "# sample input\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "\n",
        "\n",
        "# feeding those inputs to the tokenizer for tokenizing those input words and saving those tokens in the form of tensorflow tensor. \n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "print(inputs)\n",
        "\n",
        "\n",
        "# after running the code you will see two multidimensional numpy array. The first array contains tokenized words of both of the sentences and another numpy array represents the attention mask which contains 1 and zero. it is important in transformer so that while in encoder or decoder, the model is look only to the words that are tokenized. Those zero means there are no words. they are just there because of padding "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6kkYHi7oI3L",
        "outputId": "0cae1200-f89c-4685-90d5-5c24e657546b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
            "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
            "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
            "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After Tokenizing, feeding them to the same model"
      ],
      "metadata": {
        "id": "3kz3RpYeqBcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "\n",
        "\n",
        "# preparing the model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = TFAutoModel.from_pretrained(checkpoint)   # it will download the model and save it in cache\n",
        "\n",
        "\n",
        "# The inputs variable is from previous cell which contains numeric tokens\n",
        "outputs = model(inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\n",
        "\n",
        "# That (2,16,768) shape coming in output means that there are two sentences which is processed. 16 tokens/unique words. And for each of those 16 words, there is 768 length vector which represent each token. meaning if input is \"Dilip is ML engineer\" the length of vector representing \"Dilip\" token would be 768 and other token will be like the same"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8361Quap7SO",
        "outputId": "25c2b468-ea01-42a6-9316-fcc5e27e5f29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 16, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see how it looks like\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOvKByi6z3hQ",
        "outputId": "9982aa72-59cb-4b3c-e90f-e4bc5f98bddb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 16, 768), dtype=float32, numpy=\n",
              "array([[[-0.17978022,  0.23332833,  0.6320985 , ..., -0.3016663 ,\n",
              "          0.50082004,  0.14814392],\n",
              "        [ 0.2757767 ,  0.6497122 ,  0.3199771 , ..., -0.07599561,\n",
              "          0.5136171 ,  0.13292241],\n",
              "        [ 0.904585  ,  0.09851379,  0.29497236, ...,  0.33519453,\n",
              "         -0.14074168, -0.6464028 ],\n",
              "        ...,\n",
              "        [ 0.1465893 ,  0.5660602 ,  0.32352817, ..., -0.33757487,\n",
              "          0.5099777 , -0.05610804],\n",
              "        [ 0.75000465,  0.04872592,  0.17379996, ...,  0.4684146 ,\n",
              "          0.00296628, -0.6083754 ],\n",
              "        [ 0.05194408,  0.3729484 ,  0.5223324 , ...,  0.35840553,\n",
              "          0.65004265, -0.38829806]],\n",
              "\n",
              "       [[-0.29370636,  0.7282561 , -0.14972661, ..., -0.11868094,\n",
              "         -1.0226722 , -0.04215677],\n",
              "        [-0.220636  ,  0.93838435, -0.09512489, ..., -0.36431676,\n",
              "         -0.6605218 ,  0.2406973 ],\n",
              "        [-0.15360779,  0.8987497 , -0.07276388, ..., -0.21891758,\n",
              "         -0.85276   ,  0.07099427],\n",
              "        ...,\n",
              "        [-0.30174726,  0.9002208 , -0.01995084, ..., -0.10816884,\n",
              "         -0.8412144 , -0.0861433 ],\n",
              "        [-0.33384243,  0.96742386, -0.07294381, ..., -0.1951733 ,\n",
              "         -0.8181326 , -0.06339068],\n",
              "        [-0.34538025,  0.88236165, -0.0426385 , ..., -0.09926475,\n",
              "         -0.8328664 , -0.10647453]]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Sentiment Analysis using numeric representation made by transformer as input"
      ],
      "metadata": {
        "id": "2czbu74E8o4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using above transformer model, i have converted my custom texts to the high dimensional numeric representation and i am going to use that numeric representation (just like the values coming in outputs variable in above code cell) to pass them to my custom feed forward neural network followed by the sigmoid layer for binary classification of the text"
      ],
      "metadata": {
        "id": "EFIEhXbM5k7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the text input and corresponding labels\n",
        "text_input = [\"you are good\", \"you are bad\", \"I love you\",\"life is useless\"]\n",
        "labels = [1, 0, 1,0]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text_input, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the DistilBERT model and tokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize the input texts and convert them to model inputs\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Obtain the last-layer hidden states from the model\n",
        "train_outputs = model(train_encodings.input_ids)[0]\n",
        "test_outputs = model(test_encodings.input_ids)[0]\n",
        "\n",
        "\n",
        "\n",
        "sentiment_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the sentiment analysis model\n",
        "sentiment_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "# Train the sentiment analysis model\n",
        "sentiment_model.fit(train_outputs, train_labels, validation_data=(test_outputs, test_labels), epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcPkol01ge4",
        "outputId": "ce614bea-d3b5-47fb-a84b-06fe1b478131"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['dropout_19', 'classifier', 'pre_classifier']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4308 - accuracy: 1.0000 - val_loss: 0.1631 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0330 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 8.8596e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 6.1116e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d0b7728f0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with our test dataset splitted in above code cell\n",
        "print(test_texts)\n",
        "sentiment_model.predict(test_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11VhNxqP1mYn",
        "outputId": "585db30a-cb60-4602-f250-7c97b709c209"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1d0ce32f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['you are bad']\n",
            "1/1 [==============================] - 0s 67ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00133191]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Testing with our custom input. For testing with below texts i have to first convert them into the meaninful representation with the same transformer model i have used above. As my actual model \"sentiment_model\" used above takes input from the numeric representation made by the transformer model.\n",
        "\n",
        "So first we will tokenize the input sentences, pass them to the transformer model for numeric representation and then pass that as input to the sentiment_model.predict() function"
      ],
      "metadata": {
        "id": "r-PCVSP67pD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input texts and convert them to model inputs\n",
        "custom_test_texts=[\n",
        "    \"I absolutely loved the movie. It was captivating from start to finish.\",\n",
        "    \"The customer service was terrible. I had a horrible experience.\",\n",
        "    \"The book was quite intriguing and kept me hooked until the last page.\",\n",
        "    \"The restaurant had an amazing ambiance and the food was delicious.\",\n",
        "    \"I was disappointed with the quality of the product. It did not meet my expectations.\",\n",
        "    \"The performance was outstanding. The actors delivered exceptional performances.\",\n",
        "    \"I found the storyline confusing and hard to follow. It lacked coherence.\",\n",
        "    \"The hotel room was dirty and poorly maintained. I wouldn't recommend staying there.\",\n",
        "    \"The concert was electrifying. The energy in the venue was incredible.\",\n",
        "    \"The software crashed multiple times. It was frustrating and caused a lot of inconvenience.\",\n",
        "    \"I thought the experience will be worst, bad, and horrible but it ended up really amazing and good\"\n",
        "]\n",
        "validation_encodings = tokenizer(custom_test_texts, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "\n",
        "# Obtain the last-layer hidden states from the model\n",
        "validation_outputs = model(validation_encodings.input_ids)[0]\n",
        "\n",
        "\n",
        "predictions=sentiment_model.predict(validation_outputs)\n",
        "\n",
        "\n",
        "\n",
        "for sentence, prediction in zip(custom_test_texts, predictions):\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSX4X3G32aNj",
        "outputId": "f3e85b0e-8764-4631-c402-17d4c7d47f06"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1d0ce32f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 74ms/step\n",
            "Sentence: I absolutely loved the movie. It was captivating from start to finish.\n",
            "Sentiment: Positive\n",
            "Sentence: The customer service was terrible. I had a horrible experience.\n",
            "Sentiment: Negative\n",
            "Sentence: The book was quite intriguing and kept me hooked until the last page.\n",
            "Sentiment: Positive\n",
            "Sentence: The restaurant had an amazing ambiance and the food was delicious.\n",
            "Sentiment: Positive\n",
            "Sentence: I was disappointed with the quality of the product. It did not meet my expectations.\n",
            "Sentiment: Negative\n",
            "Sentence: The performance was outstanding. The actors delivered exceptional performances.\n",
            "Sentiment: Positive\n",
            "Sentence: I found the storyline confusing and hard to follow. It lacked coherence.\n",
            "Sentiment: Negative\n",
            "Sentence: The hotel room was dirty and poorly maintained. I wouldn't recommend staying there.\n",
            "Sentiment: Negative\n",
            "Sentence: The concert was electrifying. The energy in the venue was incredible.\n",
            "Sentiment: Positive\n",
            "Sentence: The software crashed multiple times. It was frustrating and caused a lot of inconvenience.\n",
            "Sentiment: Negative\n",
            "Sentence: I thought the experience will be worst, bad, and horrible but it ended up really amazing and good\n",
            "Sentiment: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing with hugging face sequence classification model"
      ],
      "metadata": {
        "id": "RZMcamw488KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "#this inputs variable is from very above code cells where we have done tokenization to our input sentences\n",
        "outputs = model(inputs)\n",
        "print(outputs.logits.shape)\n",
        "\n",
        "\n",
        "\n",
        "# the logit scores will come as output from this model. for each of the sentence there will be two scores. (for our two input sentences there will be 2*2 dimensional matrix)\n",
        "\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Z168UC9Fo0",
        "outputId": "885207df-d3ff-47ea-973d-c86a4284a8d9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_291']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n",
            "tf.Tensor(\n",
            "[[-1.5606955  1.6122805]\n",
            " [ 4.169232  -3.3464477]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Postprocessing: now let's convert this logits score to the probability score and see the respective label associated with those probability sores."
      ],
      "metadata": {
        "id": "tle-8Fq299Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probability_scores = tf.math.softmax(outputs.logits, axis=-1)\n",
        "print(probability_scores)\n",
        "# To get the labels corresponding to each position, we can inspect the id2label attribute of the model config:\n",
        "print(model.config.id2label)\n",
        "\n",
        "# so for first sentence there are two labels, negative and positive and for second sentence also"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whhm9QMf965f",
        "outputId": "995ad813-a96e-4a87-9b4b-eb02a8f69545"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[4.0195443e-02 9.5980453e-01]\n",
            " [9.9945587e-01 5.4418371e-04]], shape=(2, 2), dtype=float32)\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    }
  ]
}